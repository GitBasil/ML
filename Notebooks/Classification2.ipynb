{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.38\n",
      "Training Accuracy: 85.95%\n",
      "Validation Loss: 0.41\n",
      "Validation Accuracy: 83.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1007\n",
      "           1       0.00      0.00      0.00       193\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.42      0.50      0.46      1200\n",
      "weighted avg       0.70      0.84      0.77      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize and fit the model on the training set\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.39\n",
      "Training Accuracy: 85.95%\n",
      "Validation Loss: 0.43\n",
      "Validation Accuracy: 83.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1007\n",
      "           1       0.00      0.00      0.00       193\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.42      0.50      0.46      1200\n",
      "weighted avg       0.70      0.84      0.77      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize and fit the model on the training set\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generalized_linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.38\n",
      "Training Accuracy: 85.89%\n",
      "Validation Loss: 0.43\n",
      "Validation Accuracy: 83.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1007\n",
      "           1       0.00      0.00      0.00       193\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.42      0.50      0.46      1200\n",
      "weighted avg       0.70      0.84      0.77      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_val = sm.add_constant(X_val)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Initialize and fit the GLM model on the training set\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "result = model.fit()\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train_proba = result.predict(X_train)\n",
    "y_pred_train = (y_pred_train_proba > 0.5).astype(int)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val_proba = result.predict(X_val)\n",
    "y_pred_val = (y_pred_val_proba > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.377464\n",
      "         Iterations 7\n",
      "Training Loss: 0.38\n",
      "Training Accuracy: 85.89%\n",
      "Validation Loss: 0.43\n",
      "Validation Accuracy: 83.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1007\n",
      "           1       0.00      0.00      0.00       193\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.42      0.50      0.46      1200\n",
      "weighted avg       0.70      0.84      0.77      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_val = sm.add_constant(X_val)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Initialize and fit the Logistic Regression model on the training set\n",
    "model = sm.Logit(y_train, X_train)\n",
    "result = model.fit()\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train_proba = result.predict(X_train)\n",
    "y_pred_train = (y_pred_train_proba > 0.5).astype(int)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val_proba = result.predict(X_val)\n",
    "y_pred_val = (y_pred_val_proba > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8144 - loss: 0.4501 - val_accuracy: 0.8392 - val_loss: 0.4130\n",
      "Epoch 2/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8589 - loss: 0.3544 - val_accuracy: 0.8392 - val_loss: 0.4039\n",
      "Epoch 3/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8602 - loss: 0.3373 - val_accuracy: 0.8400 - val_loss: 0.3808\n",
      "Epoch 4/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8632 - loss: 0.3140 - val_accuracy: 0.8367 - val_loss: 0.3646\n",
      "Epoch 5/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8706 - loss: 0.2868 - val_accuracy: 0.8408 - val_loss: 0.3451\n",
      "Epoch 6/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.2745 - val_accuracy: 0.8458 - val_loss: 0.3420\n",
      "Epoch 7/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8879 - loss: 0.2550 - val_accuracy: 0.8500 - val_loss: 0.3216\n",
      "Epoch 8/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.2404 - val_accuracy: 0.8558 - val_loss: 0.3074\n",
      "Epoch 9/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9105 - loss: 0.2238 - val_accuracy: 0.8650 - val_loss: 0.3003\n",
      "Epoch 10/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9185 - loss: 0.1971 - val_accuracy: 0.8650 - val_loss: 0.2832\n",
      "Epoch 11/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9329 - loss: 0.1794 - val_accuracy: 0.8883 - val_loss: 0.2641\n",
      "Epoch 12/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9472 - loss: 0.1668 - val_accuracy: 0.8817 - val_loss: 0.2580\n",
      "Epoch 13/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9451 - loss: 0.1541 - val_accuracy: 0.8958 - val_loss: 0.2335\n",
      "Epoch 14/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9579 - loss: 0.1322 - val_accuracy: 0.8967 - val_loss: 0.2220\n",
      "Epoch 15/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.1276 - val_accuracy: 0.9050 - val_loss: 0.2260\n",
      "Epoch 16/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9612 - loss: 0.1167 - val_accuracy: 0.9058 - val_loss: 0.2098\n",
      "Epoch 17/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.1033 - val_accuracy: 0.9067 - val_loss: 0.2140\n",
      "Epoch 18/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0933 - val_accuracy: 0.9217 - val_loss: 0.1945\n",
      "Epoch 19/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9815 - loss: 0.0775 - val_accuracy: 0.9100 - val_loss: 0.1870\n",
      "Epoch 20/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0693 - val_accuracy: 0.9158 - val_loss: 0.1895\n",
      "Epoch 21/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0667 - val_accuracy: 0.9283 - val_loss: 0.1687\n",
      "Epoch 22/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9895 - loss: 0.0548 - val_accuracy: 0.9325 - val_loss: 0.1685\n",
      "Epoch 23/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9888 - loss: 0.0523 - val_accuracy: 0.9358 - val_loss: 0.1550\n",
      "Epoch 24/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0470 - val_accuracy: 0.9400 - val_loss: 0.1608\n",
      "Epoch 25/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9924 - loss: 0.0420 - val_accuracy: 0.9300 - val_loss: 0.1772\n",
      "Epoch 26/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0375 - val_accuracy: 0.9475 - val_loss: 0.1446\n",
      "Epoch 27/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9952 - loss: 0.0330 - val_accuracy: 0.9442 - val_loss: 0.1447\n",
      "Epoch 28/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9976 - loss: 0.0297 - val_accuracy: 0.9392 - val_loss: 0.1539\n",
      "Epoch 29/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0273 - val_accuracy: 0.9467 - val_loss: 0.1362\n",
      "Epoch 30/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0225 - val_accuracy: 0.9483 - val_loss: 0.1330\n",
      "Epoch 31/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0215 - val_accuracy: 0.9467 - val_loss: 0.1365\n",
      "Epoch 32/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0196 - val_accuracy: 0.9492 - val_loss: 0.1360\n",
      "Epoch 33/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0178 - val_accuracy: 0.9542 - val_loss: 0.1233\n",
      "Epoch 34/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0148 - val_accuracy: 0.9542 - val_loss: 0.1324\n",
      "Epoch 35/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0144 - val_accuracy: 0.9508 - val_loss: 0.1397\n",
      "Epoch 36/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0124 - val_accuracy: 0.9525 - val_loss: 0.1247\n",
      "Epoch 37/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.9550 - val_loss: 0.1329\n",
      "Epoch 38/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9450 - val_loss: 0.1502\n",
      "Epoch 39/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.9558 - val_loss: 0.1235\n",
      "Epoch 40/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9558 - val_loss: 0.1285\n",
      "Epoch 41/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 0.9517 - val_loss: 0.1408\n",
      "Epoch 42/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.9542 - val_loss: 0.1410\n",
      "Epoch 43/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.9625 - val_loss: 0.1235\n",
      "Epoch 44/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.9575 - val_loss: 0.1274\n",
      "Epoch 45/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9550 - val_loss: 0.1357\n",
      "Epoch 46/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9608 - val_loss: 0.1306\n",
      "Epoch 47/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.9608 - val_loss: 0.1315\n",
      "Epoch 48/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9642 - val_loss: 0.1236\n",
      "Epoch 49/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9600 - val_loss: 0.1286\n",
      "Epoch 50/50\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9667 - val_loss: 0.1146\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Training Loss: 0.00\n",
      "Training Accuracy: 100.00%\n",
      "Validation Loss: 0.11\n",
      "Validation Accuracy: 96.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1007\n",
      "           1       0.90      0.89      0.90       193\n",
      "\n",
      "    accuracy                           0.97      1200\n",
      "   macro avg       0.94      0.94      0.94      1200\n",
      "weighted avg       0.97      0.97      0.97      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# One-hot encode the target variable if it's categorical\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train_proba = model.predict(X_train)\n",
    "y_pred_train = y_pred_train_proba.argmax(axis=1)\n",
    "train_accuracy = accuracy_score(y_train.argmax(axis=1), y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val_proba = model.predict(X_val)\n",
    "y_pred_val = y_pred_val_proba.argmax(axis=1)\n",
    "val_accuracy = accuracy_score(y_val.argmax(axis=1), y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val.argmax(axis=1), y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.17\n",
      "Training Accuracy: 94.71%\n",
      "Validation Loss: 0.29\n",
      "Validation Accuracy: 88.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      1007\n",
      "           1       0.65      0.61      0.63       193\n",
      "\n",
      "    accuracy                           0.89      1200\n",
      "   macro avg       0.79      0.77      0.78      1200\n",
      "weighted avg       0.88      0.89      0.88      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the KNeighborsClassifier model on the training set\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.38\n",
      "Training Accuracy: 85.91%\n",
      "Validation Loss: 0.43\n",
      "Validation Accuracy: 83.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1007\n",
      "           1       0.00      0.00      0.00       193\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.42      0.50      0.46      1200\n",
      "weighted avg       0.70      0.84      0.77      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the LinearDiscriminantAnalysis model on the training set\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.31\n",
      "Training Accuracy: 86.75%\n",
      "Validation Loss: 0.37\n",
      "Validation Accuracy: 83.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91      1007\n",
      "           1       0.49      0.28      0.36       193\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.68      0.61      0.63      1200\n",
      "weighted avg       0.81      0.84      0.82      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the QuadraticDiscriminantAnalysis model on the training set\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m RBF(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m GaussianProcessClassifier(kernel\u001b[38;5;241m=\u001b[39mkernel, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Make predictions on the training set and calculate loss\u001b[39;00m\n\u001b[1;32m     36\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py:742\u001b[0m, in \u001b[0;36mGaussianProcessClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown multi-class mode \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class)\n\u001b[0;32m--> 742\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood_value_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    746\u001b[0m         [\n\u001b[1;32m    747\u001b[0m             estimator\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood()\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[1;32m    749\u001b[0m         ]\n\u001b[1;32m    750\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py:229\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[1;32m    228\u001b[0m optima \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constrained_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m ]\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# theta\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_restarts_optimizer \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py:474\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace._constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constrained_optimization\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin_l_bfgs_b\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 474\u001b[0m         opt_res \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m         _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res)\n\u001b[1;32m    478\u001b[0m         theta_opt, func_min \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py:220\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit.<locals>.obj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n\u001b[0;32m--> 220\u001b[0m         lml, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_marginal_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlml, \u001b[38;5;241m-\u001b[39mgrad\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py:394\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.log_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# XXX: Get rid of the np.diag() in the next line\u001b[39;00m\n\u001b[1;32m    393\u001b[0m R \u001b[38;5;241m=\u001b[39m W_sr[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m cho_solve((L, \u001b[38;5;28;01mTrue\u001b[39;00m), np\u001b[38;5;241m.\u001b[39mdiag(W_sr))  \u001b[38;5;66;03m# Line 7\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_sr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Line 8\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# Line 9: (use einsum to compute np.diag(C.T.dot(C))))\u001b[39;00m\n\u001b[1;32m    396\u001b[0m s_2 \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mdiag(K) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij, ij -> j\u001b[39m\u001b[38;5;124m\"\u001b[39m, C, C))\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;241m*\u001b[39m (pi \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m pi) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m pi))\n\u001b[1;32m    400\u001b[0m )  \u001b[38;5;66;03m# third derivative\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ai_projects/proj_1/lib/python3.9/site-packages/scipy/linalg/_basic.py:224\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b, sym_pos, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)\u001b[0m\n\u001b[1;32m    222\u001b[0m lu, ipvt, info \u001b[38;5;241m=\u001b[39m getrf(a1, overwrite_a\u001b[38;5;241m=\u001b[39moverwrite_a)\n\u001b[1;32m    223\u001b[0m _solve_check(n, info)\n\u001b[0;32m--> 224\u001b[0m x, info \u001b[38;5;241m=\u001b[39m \u001b[43mgetrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipvt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m _solve_check(n, info)\n\u001b[1;32m    227\u001b[0m rcond, info \u001b[38;5;241m=\u001b[39m gecon(lu, anorm, norm\u001b[38;5;241m=\u001b[39mnorm)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the GaussianProcessClassifier model on the training set\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "model = GaussianProcessClassifier(kernel=kernel, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.63\n",
      "Training Accuracy: 87.29%\n",
      "Validation Loss: 0.64\n",
      "Validation Accuracy: 84.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92      1007\n",
      "           1       0.63      0.14      0.23       193\n",
      "\n",
      "    accuracy                           0.85      1200\n",
      "   macro avg       0.74      0.56      0.57      1200\n",
      "weighted avg       0.82      0.85      0.81      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the AdaBoostClassifier model on the training set\n",
    "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00\n",
      "Training Accuracy: 100.00%\n",
      "Validation Loss: 1.50\n",
      "Validation Accuracy: 95.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      1007\n",
      "           1       0.80      0.99      0.88       193\n",
      "\n",
      "    accuracy                           0.96      1200\n",
      "   macro avg       0.90      0.97      0.93      1200\n",
      "weighted avg       0.97      0.96      0.96      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the DecisionTreeClassifier model on the training set\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.01\n",
      "Training Accuracy: 100.00%\n",
      "Validation Loss: 0.05\n",
      "Validation Accuracy: 99.17%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1007\n",
      "           1       0.95      1.00      0.97       193\n",
      "\n",
      "    accuracy                           0.99      1200\n",
      "   macro avg       0.98      1.00      0.98      1200\n",
      "weighted avg       0.99      0.99      0.99      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baselabubaker/Projects/ai_projects/proj_1/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [14:21:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ds3.csv')\n",
    "\n",
    "# Convert categorical variables to numeric variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the XGBClassifier model on the training set\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate loss\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_loss = log_loss(y_train, y_pred_train_proba)\n",
    "\n",
    "# Make predictions on the validation set and calculate loss\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_val_proba = model.predict_proba(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_val_proba)\n",
    "\n",
    "# Print training and validation loss and accuracy\n",
    "print(\"Training Loss: %.2f\" % train_loss)\n",
    "print(\"Training Accuracy: %.2f%%\" % (train_accuracy * 100.0))\n",
    "print(\"Validation Loss: %.2f\" % val_loss)\n",
    "print(\"Validation Accuracy: %.2f%%\" % (val_accuracy * 100.0))\n",
    "\n",
    "# Print precision, recall, and F1 score for the validation set\n",
    "print(classification_report(y_val, y_pred_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Kernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
