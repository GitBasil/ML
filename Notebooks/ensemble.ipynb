{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanadv/MLCourse/blob/main/Lesson_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRMW_gTdhW-g",
        "outputId": "e93632ed-4560-44da-d95c-3eb930859db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression: 0.98\n",
            "RandomForestClassifier: 1.00\n",
            "SVC: 0.98\n",
            "VotingClassifier: 1.00\n"
          ]
        }
      ],
      "source": [
        "#5.1 VoterClassifer\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the classifiers\n",
        "log_clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = make_pipeline(StandardScaler(), SVC(gamma='scale', probability=True, random_state=42))\n",
        "\n",
        "# Initialize the voting classifier with soft voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Train the voting classifier and the individual classifiers\n",
        "voting_clf.fit(X_train, y_train)\n",
        "log_clf.fit(X_train, y_train)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate each classifier's accuracy\n",
        "classifiers = [log_clf, rnd_clf, svm_clf, voting_clf]\n",
        "for clf in classifiers:\n",
        "    y_pred = clf.predict(X_test)\n",
        "    clf_name = clf.__class__.__name__ if clf.__class__.__name__ != 'Pipeline' else clf.steps[-1][1].__class__.__name__\n",
        "    print(f\"{clf_name}: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UvDjQrFhnvU",
        "outputId": "a1e06293-25f6-4561-e210-2531504e5646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier: 0.96\n",
            "DecisionTreeClassifier: 0.94\n",
            "GradientBoostingClassifier: 0.96\n",
            "VotingClassifier: 0.96\n"
          ]
        }
      ],
      "source": [
        "#5.2 VoterClassifer\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the classifiers\n",
        "knn_clf = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Initialize the voting classifier with soft voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('knn', knn_clf), ('dt', dt_clf), ('gb', gb_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Train the voting classifier and the individual classifiers\n",
        "for clf in (knn_clf, dt_clf, gb_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate each classifier's accuracy\n",
        "#Voting Classifier will predict all the test data using th voting of all of the models, that's why it;s scoring .96 her and 1 in the prev\n",
        "classifiers = [knn_clf, dt_clf, gb_clf, voting_clf]\n",
        "for clf in classifiers:\n",
        "    y_pred = clf.predict(X_test)\n",
        "    clf_name = clf.__class__.__name__ if clf.__class__.__name__ != 'Pipeline' else clf.steps[-1][1].__class__.__name__\n",
        "    print(f\"{clf_name}: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ft_u4pIjhKX",
        "outputId": "942154b4-7cd4-4002-e4a0-940cc88118f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.94\n"
          ]
        }
      ],
      "source": [
        "#5.3 Bagging\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the base classifier\n",
        "base_dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize the BaggingClassifier with Decision Tree Classifier as the base estimator\n",
        "#Change bootstrap to true\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_dt_clf,bootstrap=False, n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the BaggingClassifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm6YgDHmm9VA",
        "outputId": "c5abd79f-422a-4a15-f0f0-81834a481a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier Accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "#5.4 Random Forests\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the RandomForestClassifier\n",
        "random_forest_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = random_forest_clf.predict(X_test)\n",
        "print(f\"RandomForestClassifier Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJrAVrcjp8Uy",
        "outputId": "fba9bee9-d6b3-444d-f88c-40bbf61ddf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier Accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "#5.5 adaboost\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ada_clf = AdaBoostClassifier(\n",
        "         DecisionTreeClassifier(max_depth=2), n_estimators=200,\n",
        "         algorithm=\"SAMME.R\", learning_rate=0.5)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "print(f\"AdaBoostClassifier Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N3yI68HwyKG",
        "outputId": "3c309ccd-fb9f-4a43-87f4-3e69aa97d560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting Accuracy: 0.95\n"
          ]
        }
      ],
      "source": [
        "#5.5 Gradient Boosting\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)\n",
        "# residual of the actual value minus the prediction\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X, y2)\n",
        "# residual of the y2 value minus the prediction\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X, y3)\n",
        "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
        "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "# Now calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuHEn4r7zl43"
      },
      "source": [
        "Gradient Boosting is an ensemble technique that builds models sequentially, each new model correcting errors made by previously trained models. Instead of using pre-built functions, this code manually implements gradient boosting using DecisionTreeRegressors.\n",
        "\n",
        "First Model (tree_reg1): A decision tree regressor is trained on the original dataset. This model aims to predict the target variable directly.\n",
        "Second Model (tree_reg2): The first model's residuals (differences between the actual and predicted values) are calculated. A second decision tree regressor is trained on these residuals. Its goal is to correct the errors of the first model.\n",
        "Third Model (tree_reg3): Similarly, the second model's residuals are computed, and a third decision tree regressor is trained to correct the errors of the combined first and second models.\n",
        "Each decision tree has a max_depth of 2, which controls the complexity of the model. A shallow depth helps prevent overfitting.\n",
        "\n",
        "Prediction and Classification\n",
        "Prediction: The predictions from all three models are summed up to get the final prediction. This cumulative prediction approach leverages the strength of each model, focusing on correcting the predecessor's mistakes.\n",
        "Conversion to Binary Labels: Since the original problem is a classification task, but the models predict continuous values, a threshold of 0.5 is used to convert these values into binary labels (0 or 1).\n",
        "\n",
        "\n",
        "\n",
        "The line y2 = y - tree_reg1.predict(X) plays a critical role in the gradient boosting algorithm's iterative approach to model improvement. Let's break down what this line means and why it's essential:\n",
        "\n",
        "Gradient Boosting Concept\n",
        "Gradient Boosting constructs a model in a stage-wise fashion. It begins with a base model and sequentially adds new models that correct the previous models' errors. The core idea is to improve the prediction iteratively by focusing specifically on the parts where the current ensemble of models performs poorly.\n",
        "\n",
        "The Specific Line\n",
        "tree_reg1.predict(X): This part of the line uses the first decision tree regressor (tree_reg1) to make predictions on the entire dataset (X). The output of this prediction is a set of continuous values corresponding to the initial model's estimation of the target variable y.\n",
        "y - tree_reg1.predict(X): The actual values of the target variable (y) are then subtracted from these predicted values. This operation produces the residuals or errors of the first model. These residuals represent the amount by which the model's predictions deviate from the actual values.\n",
        "Purpose of Calculating y2\n",
        "Focus on Mistakes: By calculating y2, the algorithm identifies where the first model, tree_reg1, made its mistakes. Positive values in y2 indicate that the model's predictions were too low, while negative values indicate predictions that were too high.\n",
        "Training the Next Model: y2 becomes the target variable for the second decision tree regressor (tree_reg2). Essentially, tree_reg2 is trained not to predict the original target directly but to predict how much tree_reg1's predictions need to be corrected. This way, tree_reg2 focuses on the errors made by tree_reg1, aiming to reduce these errors in the overall ensemble prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suGMmbN8xwrp",
        "outputId": "5961a2e9-32ee-45c8-87d4-4a51e0b8a51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stacking Model Accuracy: 0.9825\n"
          ]
        }
      ],
      "source": [
        "# 5.6 StackingClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base-level models\n",
        "base_models = [\n",
        "    ('svc', SVC(probability=True, random_state=42)),\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "]\n",
        "\n",
        "# Define the meta-model\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Stacking Model Accuracy: {accuracy:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM4s6HOPP0mL0quXPlTDbII",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "My Kernel",
      "language": "python",
      "name": "mykernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
