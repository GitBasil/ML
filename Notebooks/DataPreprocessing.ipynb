{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkiTHUtJ9Sfm",
        "outputId": "344e6a4d-6a95-47b5-9e1d-189526f896df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Data:\n",
            "     Age   Salary         City\n",
            "5  33.0  64000.0  Los Angeles\n",
            "2  29.0  54000.0     New York\n",
            "4  32.0      NaN      Chicago\n",
            "3   NaN  58000.0      Chicago\n",
            " Test Data:\n",
            "     Age   Salary         City\n",
            "0  25.0  50000.0     New York\n",
            "1  27.0  52000.0  Los Angeles\n",
            "Preprocessed Training Data:\n",
            " [[ 1.13227703  1.4985373   0.          1.          0.        ]\n",
            " [-1.58518785 -1.31122014  0.          0.          1.        ]\n",
            " [ 0.45291081  0.          1.          0.          0.        ]\n",
            " [ 0.         -0.18731716  1.          0.          0.        ]]\n",
            "Preprocessed Test Data:\n",
            " [[-4.30265273 -2.43512311  0.          0.          1.        ]\n",
            " [-2.94392029 -1.87317162  0.          1.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# 2.1 Data Preprocessing for Classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Age': [25, 27, 29, np.nan, 32, 33, np.nan],\n",
        "    'Salary': [50000, 52000, 54000, 58000, np.nan, 64000, 66000],\n",
        "    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Chicago', 'Los Angeles', np.nan]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Splitting the dataset into features and target variable for illustration\n",
        "X = df.dropna(subset=['City'])  # Dropping rows where 'City' is NaN for this example\n",
        "y = [0, 1, 0, 1, 1, 0]  # Dummy target variable\n",
        "\n",
        "# Splitting into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining preprocessing for numerical columns (impute missing values then scale)\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Defining preprocessing for categorical columns (impute missing values then apply one-hot encoding)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combining preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, ['Age', 'Salary']),\n",
        "        ('cat', categorical_transformer, ['City'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Applying the preprocessing to the training data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# The preprocessed data is now ready for model training\n",
        "# This is a numpy array, you can convert it back to a DataFrame if needed for better readability\n",
        "\n",
        "print(\" Training Data:\\n\", X_train)\n",
        "print(\" Test Data:\\n\", X_test)\n",
        "\n",
        "print(\"Preprocessed Training Data:\\n\", X_train_preprocessed)\n",
        "\n",
        "print(\"Preprocessed Test Data:\\n\", X_test_preprocessed)\n",
        "\n",
        "\n",
        "#  1. `pd.DataFrame.dropna(subset=['City'])`\n",
        "# - Parameters:\n",
        "#   - `subset`: Column names to consider for identifying rows with missing values. Rows with NaN in these columns get dropped.\n",
        "#   - `how`: Determines if row/column is removed from DataFrame when we have at least one NA or all NA. Values are `'any'` or `'all'`. Default is `'any'`.\n",
        "#   - `inplace`: If `True`, do operation inplace and return None. Default is `False`.\n",
        "\n",
        "# - Alternatives:\n",
        "#   - If you want to drop rows where all specified columns are NaN, use `how='all'`.\n",
        "#   - To apply the operation directly to the DataFrame without creating a copy, use `inplace=True`.\n",
        "\n",
        "#  2. `train_test_split(X, y, test_size=0.2, random_state=42)`\n",
        "# - Parameters:\n",
        "#   - `X, y`: Arrays or matrices containing the dataset to split.\n",
        "#   - `test_size`: Represents the proportion of the dataset to include in the test split. Can be an int (absolute number of test samples) or a float (fraction of the dataset). Default is `None`.\n",
        "#   - `random_state`: Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.\n",
        "\n",
        "# - Alternatives:\n",
        "#   - `train_size`: Complement of `test_size`. If both are None, it will set to the default value of 0.25.\n",
        "#   - `shuffle`: Whether or not to shuffle the data before splitting. Can be useful for time-series data. Default is `True`.\n",
        "#   - `stratify`: If not None, data is split in a stratified fashion, using this as the class labels. Helps in maintaining the percentage of samples for each class.\n",
        "\n",
        "#  3. `SimpleImputer(strategy='mean')`\n",
        "# - Parameters:\n",
        "#   - `strategy`: The imputation strategy. Choices are `\"mean\"`, `\"median\"`, `\"most_frequent\"`, and `\"constant\"`.\n",
        "#   - `fill_value`: When `strategy=\"constant\"`, fill_value is used to replace all occurrences of missing values. Default is `None`.\n",
        "\n",
        "# - Alternatives:\n",
        "#   - Using `strategy=\"median\"` is a good alternative for numerical data, especially when the data might have outliers that could heavily influence the mean.\n",
        "#   - For categorical data, `strategy=\"most_frequent\"` or `strategy=\"constant\"` with a specified `fill_value` (like `\"missing\"` or `0`) can be useful.\n",
        "\n",
        "#  4. `StandardScaler()`\n",
        "# - Parameters:\n",
        "#   - `with_mean`: If `True`, center the data before scaling. Default is `True`.\n",
        "#   - `with_std`: If `True`, scale the data to unit variance (or equivalently, unit standard deviation). Default is `True`.\n",
        "\n",
        "# - Alternatives:\n",
        "#   - `MinMaxScaler`: Scales features to a given range, usually between 0 and 1.\n",
        "#   - `RobustScaler`: Useful if your data contains many outliers, scales data according to the percentile range.\n",
        "\n",
        "#  5. `OneHotEncoder(handle_unknown='ignore')`\n",
        "# - Parameters:\n",
        "#   - `handle_unknown`: Options are `\"error\"` or `\"ignore\"`. Determines what happens when the encoder encounters a category not seen during fit. If `\"ignore\"`, the unknown category is ignored (encoded as all zeros).\n",
        "#   - `sparse`: Whether the transformed output is a sparse matrix or a 2D array. Default is `True`.\n",
        "\n",
        "# - Alternatives:\n",
        "#   - `LabelEncoder`: Good for encoding target labels (y) rather than input (X) features.\n",
        "#   - `OrdinalEncoder`: Transforms categorical features to ordinal integers. Useful when the categorical features have a natural order.\n",
        "\n",
        "#  6. `ColumnTransformer`\n",
        "# - Parameters:\n",
        "#   - `transformers`: List of transformers to apply. Each transformer is a tuple containing a name, transformer object, and column(s) to apply the transformer to.\n",
        "#   - `remainder`: Determines what to do with the remaining columns not explicitly selected in `transformers`. Options are `'drop'` (default), `'passthrough'`, or a transformer object to apply.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  `random_state=42` in `train_test_split`\n",
        "# The `train_test_split` function from `scikit-learn` is used to split the dataset into training and testing sets. Here, `random_state` is set to 42, which serves as a seed for the random number generator. This ensures that the split is reproducible; anyone running this code with `random_state` set to 42 will get the exact same training and testing sets. This is particularly useful for educational purposes, demonstrations, or scenarios where you want to ensure consistent results across different runs for debugging or comparison purposes.\n",
        "\n",
        "#  `X_train_preprocessed = preprocessor.fit_transform(X_train)`\n",
        "# This line applies the preprocessing steps defined in the `preprocessor` to the training data (`X_train`). The `preprocessor` is a `ColumnTransformer` that combines both numerical and categorical transformations:\n",
        "\n",
        "# - For numerical columns (`'Age'`, `'Salary'`), it first imputes missing values using the mean (with `SimpleImputer(strategy='mean')`), and then standardizes the features (with `StandardScaler()`), which scales the features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "# - For the categorical column (`'City'`), it first imputes missing values by replacing them with the most frequent category (with `SimpleImputer(strategy='most_frequent')`), and then applies one-hot encoding (with `OneHotEncoder(handle_unknown='ignore')`), which converts the categorical variable into a format that can be provided to machine learning algorithms (creating a binary column for each category).\n",
        "\n",
        "# The `fit_transform` method on the `preprocessor` does two things: it first `fit`s the transformers to the training data, learning any necessary parameters (like the mean and standard deviation for scaling, or the categories for one-hot encoding), and then `transform`s the training data according to these parameters, outputting the preprocessed data ready for model training.\n",
        "\n",
        "#  `X_test_preprocessed = preprocessor.transform(X_test)`\n",
        "# After the `preprocessor` has been fitted to the training data, it is used to transform the test data (`X_test`) using the same `transform` method. However, since the `preprocessor` has already been fitted, it uses the parameters learned from the training data (not from `X_test`). This ensures that the test data is preprocessed in exactly the same way as the training data, which is crucial for the model to make accurate predictions on the test data. This step does not involve fitting (`fit`) the `preprocessor` again to the test data, as that would lead to data leakage and overfitting.\n",
        "\n",
        "# In this workflow, the preprocessed training and test data are represented as numpy arrays. If needed for better readability or for further processing that requires a DataFrame structure, these arrays can be converted back into pandas DataFrames, although the column names for the one-hot encoded variables would need to be manually specified or extracted from the `preprocessor`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMynxSTED9SZXM2YEPiogMR",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "My Kernel",
      "language": "python",
      "name": "mykernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
